generate:
  lm_model: "gpt-4o-mini"
  output_length: 128
  num_of_examples: 144
  concept_path: "axbench/data/gemma-2-2b_20-gemmascope-res-16k.json"
  max_concepts: 16000
  dataset_category: "instruction"
  master_data_dir: "axbench/data"
  lm_use_cache: false
  seed: 42
  keep_orig_axbench_format: true

train:
  model_name: "google/gemma-2-2b-it"
  layer: 10
  component: "res"
  seed: 42
  use_bf16: true
  use_wandb: true
  wandb_project: "hypersteer"
  wandb_name: "marcomolinari4"
  run_name: "16k-fullMLPloras-L10-r4-6-layers"
  output_length: 128
  models:
    HyperSteerWeight:
      batch_size: 16
      gradient_accumulation_steps: 1
      n_epochs: 10
      lr: 0.001
      lr_decay_start_step: 1700
      lr_min: 0.0
      weight_decay: 0.0
      low_rank_dimension: 64
      intervention_positions: "all"
      binarize_dataset: false
      train_on_negative: false
      exclude_bos: true
      hypernet_name_or_path: "google/gemma-2-2b-it"
      num_hidden_layers: 8
      hypernet_initialize_from_pretrained: false
      weight_target_modules:
        - "model.layers.10.mlp.up_proj.weight"
        - "model.layers.11.mlp.up_proj.weight"
        - "model.layers.12.mlp.up_proj.weight"
        - "model.layers.13.mlp.up_proj.weight"
        - "model.layers.14.mlp.up_proj.weight"
        - "model.layers.15.mlp.up_proj.weight"
        - "model.layers.10.mlp.down_proj.weight"
        - "model.layers.11.mlp.down_proj.weight"
        - "model.layers.12.mlp.down_proj.weight"
        - "model.layers.13.mlp.down_proj.weight"
        - "model.layers.14.mlp.down_proj.weight"
        - "model.layers.15.mlp.down_proj.weight"
inference:
  use_bf16: true
  models: ["HyperSteerWeight"]
  model_name: "google/gemma-2-2b-it"
  # latent related params
  output_length: 128
  latent_num_of_examples: 36
  latent_batch_size: 16
  # steering related params
  steering_intervention_type: "weight"
  steering_model_name: "google/gemma-2-2b-it"
  steering_datasets: ["AlpacaEval"]
  steering_batch_size: 8
  steering_output_length: 128
  steering_layers: [10]
  steering_num_of_examples: 10
  steering_factors: [0.0, 1.0]
  sample_concepts: 500
  master_data_dir: "axbench/data"
  seed: 42
  lm_model: "gpt-4o-mini"
  temperature: 1.0

evaluate:
  models: ["HyperSteerWeight"]
  latent_evaluators: [
    "AUCROCEvaluator",
    "HardNegativeEvaluator",
  ]
  steering_evaluators: [
    "PerplexityEvaluator",
    "LMJudgeEvaluator",
  ]
  winrate_split_ratio: 0.5
  num_of_workers: 32
  lm_model: "gpt-4o-mini"
  run_winrate: false
  sample_concepts: 500
  sample_examples: 10
  winrate_baseline: "PromptSteering"
  master_data_dir: "axbench/data"
