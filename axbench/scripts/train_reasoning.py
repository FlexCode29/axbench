# train.py: Script for training a model using the dataset generated by the previous script.
# This script takes arguments to specify the dataset and other configurations.
#
# example launch command:
#     torchrun --nproc_per_node=NUM_GPUS axbench/scripts/train.py --config axbench/demo/sweep/train.yaml
import os
import argparse
import yaml
import json
import glob
import pickle
import torch
import shutil
import requests
import datetime
import pandas as pd
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import hf_hub_download, snapshot_download
import glob
from pathlib import Path
from args.training_args import TrainingArgs
from args.dataset_args import DatasetArgs
from axbench.utils.constants import * 
from axbench.utils.model_utils import get_prefix_length, get_suffix_length
from transformers import set_seed
import torch.distributed as dist
import sys
from torch.utils.data import DataLoader
from axbench.models.sae import save_pruned_sae
from axbench.models.hypernet.utils import prepare_df_combined

# all supported methods
import axbench

import logging

# Initialize the logger
logger = logging.getLogger(__name__)

CONFIG_FILE = "config.json"
STATE_FILE = "train_state.pkl"
METADATA_FILE = "metadata.jsonl"


def data_generator(data_dir, use_dpo_loss=False):
    """
    Generator function to read multiple data files and yield data subsets by concept_id.
    Processes files in order: train_data.parquet, train_data_0.parquet, train_data_1.parquet, etc.

    Args:
        data_dir (str): Path to the data directory.

    Yields:
        (concept_id, df_subset): A tuple containing the concept_id and subset DataFrame.
    """
    # Gather all file paths in the directory
    if use_dpo_loss:
        file_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) \
            if f.startswith('dpo_train_data') and f.endswith('.parquet') and "combined" not in f]
    else:
        file_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) \
            if f.startswith('train_data') and f.endswith('.parquet') and "combined" not in f]

    # Sort files: 'train_data.parquet' comes first, then 'train_data_X.parquet' sorted by X
    def extract_index(file_name):
        if use_dpo_loss:
            if file_name == 'dpo_train_data.parquet':
                return -1  # Ensure 'train_data.parquet' comes first
            else:
                # Extract the number X from 'train_data_X.parquet'
                return int(file_name.split('_')[-1].split('.')[0])
        else:
            if file_name == 'train_data.parquet':
                return -1  # Ensure 'train_data.parquet' comes first
            else:
                # Extract the number X from 'train_data_X.parquet'
                return int(file_name.split('_')[-1].split('.')[0])

    file_paths.sort(key=lambda x: extract_index(os.path.basename(x)))

    for file_path in file_paths:
        df = pd.read_parquet(file_path)
        concept_ids = df['concept_id'].unique()
        concept_ids.sort()
        for concept_id in concept_ids:
            if concept_id >= 0:
                # print(f"Processing concept_id {concept_id}")
                df_subset = df[df['concept_id'] == concept_id]
                yield (concept_id, df_subset)



def load_metadata(metadata_path):
    """
    Load metadata from a JSON lines file.
    """
    metadata = []
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            metadata += [data]  # Return the metadata as is
    return metadata


# --- HF snapshot conversion helpers ---

# Helper: Find the actual data root containing train-*.parquet (handles HF "data/" subdir)
# --- HF snapshot conversion helpers ---
def _resolve_hf_data_root(data_dir: str) -> str:
    if glob.glob(os.path.join(data_dir, "train-*.parquet")):
        return data_dir
    candidate = os.path.join(data_dir, "data")
    if os.path.isdir(candidate) and glob.glob(os.path.join(candidate, "train-*.parquet")):
        return candidate
    matches = glob.glob(os.path.join(data_dir, "**", "train-*.parquet"), recursive=True)
    if matches:
        return os.path.dirname(matches[0])
    return data_dir

def _find_hf_train_parquet(data_dir: str):
    root = _resolve_hf_data_root(data_dir)
    candidates = sorted(glob.glob(os.path.join(root, "train-*.parquet")))
    return candidates[0] if candidates else None

def _ensure_axbench_files_from_hf_snapshot(data_dir: str) -> None:
    root = _resolve_hf_data_root(data_dir)
    train_data_path = os.path.join(root, "train_data.parquet")
    metadata_path = os.path.join(root, "metadata.jsonl")

    if os.path.exists(train_data_path) and os.path.exists(metadata_path):
        return

    hf_parquet = _find_hf_train_parquet(data_dir)
    if hf_parquet is None:
        return

    df = pd.read_parquet(hf_parquet)
    if not os.path.exists(train_data_path):
        df.to_parquet(train_data_path, index=False)

    if not os.path.exists(metadata_path):
        concept_name_col = None
        for c in ["concept", "concept_name", "concept_str"]:
            if c in df.columns:
                concept_name_col = c
                break

        genre_col = "concept_genre" if "concept_genre" in df.columns else None
        if "concept_id" in df.columns:
            cid_col = "concept_id"
        elif "output_concept" in df.columns:
            cid_col = "output_concept"
        else:
            cid_col = None

        entries = []
        if cid_col is not None:
            concept_ids = pd.Series(df[cid_col].unique()).dropna().astype(int).tolist()
            concept_ids.sort()
            for cid in concept_ids:
                if cid < 0:
                    continue
                sub = df[df[cid_col] == cid]
                name = str(sub.iloc[0][concept_name_col]) if concept_name_col else str(cid)
                genre = str(sub.iloc[0][genre_col]) if genre_col else "unknown"
                entries.append({
                    "concept_id": int(cid),
                    "concept": name,
                    "concept_genres_map": {name: [genre]},
                })

        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
        with open(metadata_path, "w") as f:
            for e in entries:
                f.write(json.dumps(e) + "\n")


def prepare_df(
    original_df, negative_df, concept, metadata, tokenizer, 
    binarize, train_on_negative, is_chat_model, output_length, model_name, 
    max_num_of_examples=None, use_dpo_loss=False, steering_prompt_type="prepend",
    keep_orig_axbench_format=False):
    
    suffix_length, suffix_str = get_suffix_length(tokenizer)
    print(f"Suffix length for {model_name}: {suffix_length}, Suffix string: {suffix_str}")
    genre = metadata["concept_genres_map"][concept][0]
    # assign input and output containing concept with 1, otherwise 0
    positive_df = original_df[original_df["concept_genre"] == "positive"]
    negative_df = negative_df[(negative_df["concept_genre"] == genre)]
    if max_num_of_examples:
        positive_df = positive_df.head(max_num_of_examples // 2)
        negative_df = negative_df.head(max_num_of_examples // 2)
    if binarize:
        if is_chat_model:
            if model_name in HAS_SYSTEM_PROMPT_MODELS:
                def apply_chat_template(row):
                    messages = [
                        {"role": "system", "content": "You are a helpful assistant."}, 
                        {"role": "user", "content": row["input"]},
                        {"role": "assistant", "content": row["output"]}
                    ]
                    nobos = tokenizer.apply_chat_template(
                        messages, tokenize=True, add_generation_prompt=True)[1:-suffix_length]
                    return tokenizer.decode(nobos)
                positive_df = positive_df.copy()
                negative_df = negative_df.copy()
                positive_df['combined'] = positive_df.apply(apply_chat_template, axis=1)
                negative_df['combined'] = negative_df.apply(apply_chat_template, axis=1)
            else:
                def apply_chat_template(row):
                    messages = [
                        {"role": "user", "content": row["input"]},
                        {"role": "assistant", "content": row["output"]}
                    ]
                    nobos = tokenizer.apply_chat_template(
                        messages, tokenize=True, add_generation_prompt=True)[1:-suffix_length]
                    return tokenizer.decode(nobos)
                positive_df = positive_df.copy()
                negative_df = negative_df.copy()
                positive_df['combined'] = positive_df.apply(apply_chat_template, axis=1)
                negative_df['combined'] = negative_df.apply(apply_chat_template, axis=1)
        else:
            positive_df = positive_df.copy()
            negative_df = negative_df.copy()
            positive_df['combined'] = positive_df['input'] + positive_df['output']
            negative_df['combined'] = negative_df['input'] + negative_df['output']
        positive_df = pd.DataFrame(positive_df[['combined']]).rename(columns={'combined': 'input'})
        negative_df = pd.DataFrame(negative_df[['combined']]).rename(columns={'combined': 'input'})
        positive_df["labels"] = 1
        negative_df["labels"] = 0
        return pd.concat([positive_df, negative_df], axis=0)
    else:
        # if not binarizing, we need to apply the chat template to the input. It becomes a standard instruction tuning task.
        if not use_dpo_loss and train_on_negative:
            all_df = pd.concat([positive_df, negative_df], axis=0)
        else:
            # for DPO, we only use positive examples.
            all_df = positive_df
        if is_chat_model:
            system_messages = []
            if model_name in HAS_SYSTEM_PROMPT_MODELS:
                system_messages = [{"role": "system", "content": "You are a helpful assistant."}]
            
            def apply_chat_template(df, column_name):
                def template_function(row):
                    messages = system_messages + [{"role": "user", "content": row[column_name]}]
                    nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]
                    return tokenizer.decode(nobos)
                df[column_name] = df.apply(template_function, axis=1)

            apply_chat_template(all_df, "input")
            if use_dpo_loss:
                if f"{steering_prompt_type}_steered_input" in all_df.columns:
                    apply_chat_template(all_df, f"{steering_prompt_type}_steered_input")

            # Add EOS prefix tokens by default. The truncation at data collator will take care of the rest.
            def apply_output_template(df, column_name):
                def template_function(row):
                    return row[column_name] + suffix_str
                df[column_name] = df.apply(template_function, axis=1)
            
            # AxBench has much shorter outputs. We follow the original AxBench format.
            if not keep_orig_axbench_format:
                # Apply the template to all output columns
                for column in ["output", "winning_output", "losing_output", "prepend_steered_output", "blend_in_steered_output"]:
                    if column in all_df.columns:
                        apply_output_template(all_df, column)

            # Print sample row data
            print("\n=== Sample Row Data ===")
            sample_row = all_df.iloc[0]
            for column in sample_row.index:
                print(f"\n{column}:")
                print("-" * (len(column) + 1))
                print(f"{sample_row[column]}")
            print("=====================\n")

        return all_df # do nothing, the task will be standard instruction tuning.


def partition_list(lst, n):
    """
    Partition a list into n approximately equal slices.

    Args:
        lst (list): The list to partition.
        n (int): The number of partitions.

    Returns:
        list of lists: A list containing n sublists.
    """
    k, m = divmod(len(lst), n)
    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]


def load_state(dump_dir, rank):
    """
    Load the state from a file if it exists.
    """
    state_path = os.path.join(f"{dump_dir}", f"{STATE_FILE}_rank_{rank}")
    if os.path.exists(state_path):
        with open(state_path, "rb") as f:
            return pickle.load(f)
    return None


def save_state(dump_dir, state, concept_metadata, rank):
    dump_dir = Path(dump_dir)
    dump_dir.mkdir(parents=True, exist_ok=True)
    # Save state
    state_path = os.path.join(dump_dir, f"{STATE_FILE}_rank_{rank}")
    with open(state_path, "wb") as f:
        pickle.dump(state, f)

    # Save metadata again
    metadata_path = os.path.join(dump_dir, f"rank_{rank}_{METADATA_FILE}")
    with open(metadata_path, "a") as f:
        f.write(json.dumps(concept_metadata) + "\n")
        
        
def train_hypersteer(args, generate_args, model_instance, tokenizer, all_df, metadata, dump_dir, rank, device, local_rank, world_size, raw_cfg):
    # Get the rank and world_size from environment variables
    # HyperSteer-only: instruction datasets may not include explicit negatives or a `category` column
    negative_df = all_df.iloc[0:0].copy()
    # Accept either HyperSteer or HyperSteerWeight
    model_name = "HyperSteerWeight" if "HyperSteerWeight" in args.models.keys() else "HyperSteer"

    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')

    # Defensive check: HyperSteerWeight requires concept_id in metadata
    if rank == 0:
        with open(metadata_path, "r") as f:
            first = json.loads(next(iter(f)))
        if "concept_id" not in first:
            raise RuntimeError(
                "metadata.jsonl is missing `concept_id`. "
                "Delete the HF-converted metadata.jsonl and rerun so it can be regenerated."
            )
    is_chat_model = True if args.model_name in CHAT_MODELS else False

    benchmark_model = getattr(axbench, model_name)(
        model_instance, tokenizer, layer=args.layer,
        training_args=args.models[model_name],
        lm_model_name=args.model_name,
        device=device, seed=args.seed,
    )

    low_rank_dimension = args.models[model_name].low_rank_dimension \
        if args.models[model_name].low_rank_dimension else 1

    prefix_length = 1 # prefix is default to 1 for all models due to theBOS token.
    if is_chat_model:
        prefix_length = get_prefix_length(tokenizer)
        logger.warning(f"Chat model prefix length: {prefix_length}")

    benchmark_model.make_model(
        mode="train",
        embed_dim=model_instance.config.hidden_size,
        low_rank_dimension=low_rank_dimension,
        num_hidden_layers=args.models[model_name].num_hidden_layers,
        dtype=torch.bfloat16 if args.use_bf16 else None,
        intervention_type=args.models[model_name].intervention_type,
        metadata_path=metadata_path,
        dump_dir=dump_dir,
        model_params=args.models[model_name],
        hypernet_name_or_path=args.models[model_name].hypernet_name_or_path,
        hypernet_initialize_from_pretrained=args.models[model_name].hypernet_initialize_from_pretrained,
    )

    full_df = all_df.copy()

    # HF reasoning datasets may not include a `category` column.
    # prepare_df_combined assumes AxBench-style positives/negatives.
    if "category" not in full_df.columns:
        logger.warning(
            "[HyperSteer] No `category` column found in dataset. "
            "Skipping prepare_df_combined and using instruction-only data."
        )
    else:
        full_df = prepare_df_combined(
            full_df, negative_df, tokenizer,
            binarize=args.models[model_name].binarize_dataset,
            train_on_negative=args.models[model_name].train_on_negative,
            is_chat_model=is_chat_model,
            output_length=generate_args.output_length,
            model_name=args.model_name,
            max_num_of_examples=args.max_num_of_examples,
        )
    # Removed: max_training_examples truncation here. It is now handled only inside HyperSteer.make_dataloader.

    # Read raw YAML config (TrainingArgs drops unknown fields)
    raw_model_cfg = (
        raw_cfg
            .get("train", {})
            .get("models", {})
            .get(model_name, {})
    )

    kwargs = {
        "prefix_length": prefix_length,
        "positions": args.models[model_name].intervention_positions,
        "exclude_bos": args.models[model_name].exclude_bos,
        "metadata_path": metadata_path,
        "world_size": world_size,
        "max_train_steps": raw_model_cfg.get("max_train_steps", None),
        "max_training_examples": raw_model_cfg.get("max_training_examples", None),
    }
    if rank == 0:
        print("[DEBUG] raw HyperSteer config:", raw_model_cfg)

    benchmark_model.train(full_df, **kwargs)
    if rank == 0:
        logger.warning("Rank 0 is merging results.")
        benchmark_model.save(dump_dir, model_name=model_name)
    

def main():
    # --- Pre-parse --config so we can load raw YAML ---
    pre_parser = argparse.ArgumentParser(add_help=False)
    pre_parser.add_argument("--config", type=str, required=True)
    pre_args, _ = pre_parser.parse_known_args()

    with open(pre_args.config, "r") as f:
        raw_cfg = yaml.safe_load(f)

    # --- Now parse structured args ---
    args = TrainingArgs(section="train")
    generate_args = DatasetArgs(section="generate")

    # Detect whether we are running under torchrun
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        dist.init_process_group(
            backend="nccl",
            init_method="env://",
            timeout=datetime.timedelta(seconds=6000),
        )
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
    else:
        # Single-process (no torch.distributed)
        rank = 0
        world_size = 1
        local_rank = 0

    # Set the device for this process
    device = torch.device(f'cuda:{local_rank}')
    torch.cuda.set_device(device)

    # Set a unique seed per rank for reproducibility
    set_seed(args.seed + rank)

    if args.overwrite_data_dir:
        odir = Path(args.overwrite_data_dir)
        if odir.exists():
            logger.warning(f"Using local dataset directory {odir}")
            args.data_dir = str(odir)
        else:
            logger.warning(f"Downloading HF dataset {args.overwrite_data_dir}")
            hf_target_dir = Path(args.dump_dir) / "hf_datasets" / args.overwrite_data_dir.replace("/", "__")
            hf_target_dir.mkdir(parents=True, exist_ok=True)
            if rank == 0:
                snapshot_download(
                    repo_id=args.overwrite_data_dir,
                    repo_type="dataset",
                    local_dir=hf_target_dir,
                    local_dir_use_symlinks=False,
                )
            dist.barrier()
            args.data_dir = str(hf_target_dir)
    else:
        args.data_dir = f"{args.dump_dir}/generate"

    # If this is an HF snapshot that doesn't include AxBench's expected filenames, convert in-place.
    if rank == 0:
        _ensure_axbench_files_from_hf_snapshot(args.data_dir)
        resolved = _resolve_hf_data_root(args.data_dir)
        logger.warning(f"HF snapshot conversion complete. Using data dir: {resolved}. Files: {os.listdir(resolved)}")
    else:
        resolved = None

    resolved_list = [resolved]
    dist.broadcast_object_list(resolved_list, src=0)
    args.data_dir = resolved_list[0]
    dist.barrier()

    # If this is an HF snapshot that doesn't include AxBench's expected filenames, convert in-place.
    if rank == 0:
        _ensure_axbench_files_from_hf_snapshot(args.data_dir)
        resolved = _resolve_hf_data_root(args.data_dir)
        logger.warning(
            f"HF snapshot conversion complete. Using data dir: {resolved}. Files present: {os.listdir(resolved)}"
        )
    else:
        resolved = None

    # Broadcast the resolved data dir from rank 0 to all ranks
    resolved_list = [resolved]

    if dist.is_available() and dist.is_initialized():
        dist.broadcast_object_list(resolved_list, src=0)

    args.data_dir = resolved_list[0]

    if dist.is_available() and dist.is_initialized():
        dist.barrier()

    # Configure the logger per rank
    logger.setLevel(logging.WARNING)  # Set the logging level as desired

    # Create a logging formatter that includes the rank
    formatter = logging.Formatter(
        fmt=f'%(asctime)s,%(msecs)03d %(levelname)-8s [Rank {rank}] [%(filename)s:%(lineno)d] %(message)s',
        datefmt='%Y-%m-%d:%H:%M:%S'
    )

    # Create a console handler and set its formatter
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    # Add the handler to the logger
    if not logger.handlers:
        logger.addHandler(console_handler)

    # Optionally, create a file handler per rank
    """
    log_file = f'log_rank_{rank}.log'
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    """

    # Load dataset and metadata
    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')
    metadata = load_metadata(metadata_path)
    df_generator = data_generator(args.data_dir, use_dpo_loss=args.use_dpo_loss)
    if args.use_dpo_loss:
        all_df = pd.read_parquet(os.path.join(args.data_dir, 'dpo_train_data.parquet'))
    else:
        all_df = pd.read_parquet(os.path.join(args.data_dir, 'train_data.parquet')) # this is needed for binarizing the dataset
    # HyperSteer-only: do not rely on explicit negatives
    negative_df = all_df.iloc[0:0].copy()
    
    df_list = list(df_generator)
    logger.warning(f"Total number of concept df loaded: {len(df_list)}")
    if args.max_concepts:
        logger.warning(f"All ranks only processing {args.max_concepts} concepts")
        df_list = df_list[:args.max_concepts]

    dump_dir = Path(args.dump_dir) / "train"
    dump_dir.mkdir(parents=True, exist_ok=True)
    
    # save pruned SAE
    sae_params = None # TODO: this is a workaround to avoid breaking the code.
    if rank == 0:
        try:
            sae_params = save_pruned_sae(metadata_path, dump_dir)
        except:
            sae_params = None

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, model_max_length=512)
    tokenizer.padding_side = "right"

    # Partition df_list among ranks
    df_list_per_rank = partition_list(df_list, world_size)
    my_df_list = df_list_per_rank[rank]

    # Load model instance onto device
    if args.use_bf16:
        logger.warning(f"Using bfloat16 for model {args.model_name}")
    model_instance = AutoModelForCausalLM.from_pretrained(
        args.model_name, torch_dtype=torch.bfloat16 if args.use_bf16 else None)
    is_chat_model = True if args.model_name in CHAT_MODELS else False
    model_instance = model_instance.eval()
    model_instance.to(device)

    if tokenizer.unk_token == None and tokenizer.pad_token == None:
        # raw llama3
        print("adding a special padding token...")
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        need_resize = True
    else:
        need_resize = False
    if need_resize:
        model_instance.resize_token_embeddings(len(tokenizer))

    prefix_length = 1 # prefix is default to 1 for all models due to theBOS token.
    if is_chat_model:
        prefix_length = get_prefix_length(tokenizer)
        logger.warning(f"Chat model prefix length: {prefix_length}")

    state = load_state(dump_dir, rank)
    last_concept_id = state.get("last_concept_id", None) if state else None
    logger.warning(f"Rank {rank} last concept_id processed: {last_concept_id}")

    # Run training for assigned concept_ids
    # logger.warning(metadata)
    
    # Baseline training skip logic: skip if only HyperSteer/HyperSteerWeight is present
    hypersteer_models = [m for m in args.models.keys() if m in ("HyperSteer", "HyperSteerWeight")]
    if set(args.models.keys()) <= set(["HyperSteer", "HyperSteerWeight"]):
        logger.warning("HyperSteer-only mode enabled: skipping per-concept baseline training.")
        logger.warning("Running HyperSteer-only training (no baselines, instruction-only data).")
    else:
        # (Baseline training logic would go here if present)
        pass

    dist.barrier()

    # Run HyperSteer/HyperSteerWeight training if present
    for model_name in ["HyperSteer", "HyperSteerWeight"]:
        if model_name in args.models.keys():
            train_hypersteer(
                args, generate_args, model_instance, tokenizer,
                all_df, metadata, dump_dir,
                rank, device, local_rank, world_size,
                raw_cfg=raw_cfg,
            )

    dist.barrier()

    # Rank 0 merges results
    if rank == 0:
        logger.warning("Rank 0 is merging results.")

        # Merging metadata (may not exist in HyperSteer-only mode)
        metadata_entries = []
        for r in range(world_size):
            rank_metadata_path = os.path.join(dump_dir, f"rank_{r}_{METADATA_FILE}")
            if not os.path.exists(rank_metadata_path):
                logger.warning(
                    f"[HyperSteer] Metadata file not found at {rank_metadata_path}. "
                    "Skipping (expected in HyperSteer-only mode)."
                )
                continue
            with open(rank_metadata_path, "r") as f:
                for line in f:
                    metadata_entry = json.loads(line)
                    metadata_entries.append(metadata_entry)

        if metadata_entries:
            merged_metadata_path = os.path.join(dump_dir, METADATA_FILE)
            with open(merged_metadata_path, "a") as f:
                for metadata_entry in metadata_entries:
                    f.write(json.dumps(metadata_entry) + "\n")
        else:
            logger.warning(
                "[HyperSteer] No per-rank metadata found. "
                "Skipping merged metadata write."
            )

        config = {"model_name": args.model_name,
                "layer": args.layer,
                "component": args.component}
        config_path = dump_dir / CONFIG_FILE
        with open(config_path, 'w') as f:
            json.dump(config, f)
        # Do not skip merging for HyperSteerWeight; merge logic applies to all

    # Finalize the process group
    if dist.is_available() and dist.is_initialized():
        dist.destroy_process_group()

    # Remove handlers to prevent duplication if the script is run multiple times
    logger.removeHandler(console_handler)
    # If file_handler is used, remove it as well
    # logger.removeHandler(file_handler)


if __name__ == "__main__":
    main()

